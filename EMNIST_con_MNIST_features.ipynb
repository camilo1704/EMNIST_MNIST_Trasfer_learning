{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST con MNIST features\n",
    "\n",
    "En esta notebook se entrena una red, utilizando los features aprendidos de la base de datos del MNIST, para clasificar las clases de la base de datos de EMNIST, para ello se divide la red en la parte de feature layers de las capas convolucionales y la parte de clasificación de las capas densas. Se copian los pesos de las capas convolucionales aprendidos de MNIST y se entrenan los pesos de las capas de clasificación, obteniendo finalmente un accuracy de 93.68% para 15 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuchuflito/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import imageio\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from skimage import transform,io\n",
    "from resizeimage import resizeimage\n",
    "import scipy.io as sio\n",
    "import scipy.misc\n",
    "from skimage.transform import rescale, resize as rs, downscale_local_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impr(word):\n",
    "    print ('\\033[1m' + word + '\\033[0m')\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 15,\n",
    "        'size': 16,\n",
    "        }\n",
    "file=sio.loadmat('emnist-letters.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=file['dataset'][0,0][0][0][0][0]\n",
    "train_labels=file['dataset'][0,0][0][0][0][1]\n",
    "test=file['dataset'][0,0][1][0][0][0]\n",
    "test_labels=file['dataset'][0,0][1][0][0][1]\n",
    "labels=[]\n",
    "for i in train_labels:\n",
    "    labels=np.append(labels,int(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=labels-1   #empieza en 1 los labels----> les resto uno, sino hay problemas con el to_categorical \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1) #tensorflow channels_last\n",
    "num_classes = 26\n",
    "feature_layers = [\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape = (28,28,1)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "classification_layers = [\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "]\n",
    "for l in feature_layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= [layer.get_weights() for layer in model.layers]\n",
    "new_model= Sequential()\n",
    "\n",
    "new_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "new_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_model.add(Dropout(0.25))\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(128, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(26, activation='softmax'))\n",
    "for i in np.arange(0,len(feature_layers),1):\n",
    "        new_model.layers[i].set_weights(weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "train_labels = keras.utils.to_categorical(train_labels,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "124800/124800 [==============================] - 299s 2ms/step - loss: 0.7552 - acc: 0.7730\n",
      "Epoch 2/15\n",
      "124800/124800 [==============================] - 284s 2ms/step - loss: 0.4104 - acc: 0.8703\n",
      "Epoch 3/15\n",
      "124800/124800 [==============================] - 314s 3ms/step - loss: 0.3466 - acc: 0.8893\n",
      "Epoch 4/15\n",
      "124800/124800 [==============================] - 300s 2ms/step - loss: 0.3109 - acc: 0.9009\n",
      "Epoch 5/15\n",
      "124800/124800 [==============================] - 298s 2ms/step - loss: 0.2862 - acc: 0.9071\n",
      "Epoch 6/15\n",
      "124800/124800 [==============================] - 286s 2ms/step - loss: 0.2675 - acc: 0.9133\n",
      "Epoch 7/15\n",
      "124800/124800 [==============================] - 286s 2ms/step - loss: 0.2538 - acc: 0.9158\n",
      "Epoch 8/15\n",
      "124800/124800 [==============================] - 291s 2ms/step - loss: 0.2409 - acc: 0.9197\n",
      "Epoch 9/15\n",
      "124800/124800 [==============================] - 288s 2ms/step - loss: 0.2331 - acc: 0.9226\n",
      "Epoch 10/15\n",
      "124800/124800 [==============================] - 329s 3ms/step - loss: 0.2208 - acc: 0.9267\n",
      "Epoch 11/15\n",
      "124800/124800 [==============================] - 287s 2ms/step - loss: 0.2156 - acc: 0.9269\n",
      "Epoch 12/15\n",
      "124800/124800 [==============================] - 286s 2ms/step - loss: 0.2103 - acc: 0.9287\n",
      "Epoch 13/15\n",
      "124800/124800 [==============================] - 296s 2ms/step - loss: 0.2003 - acc: 0.9316\n",
      "Epoch 14/15\n",
      "124800/124800 [==============================] - 286s 2ms/step - loss: 0.1962 - acc: 0.9332\n",
      "Epoch 15/15\n",
      "124800/124800 [==============================] - 287s 2ms/step - loss: 0.1901 - acc: 0.9347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f510bd6ad68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.reshape(train.shape[0], 28, 28, 1).astype('float32')/255\n",
    "new_model.fit(train,train_labels,batch_size=128*4, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nmodel=load_model('modelo_EMNIST.h5')\n",
    "new_model.save('modelo_EMNIST_ft_MNIST_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reshape(test.shape[0], img_rows, img_cols, 1).astype('float32')/255\n",
    "prediction=new_model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m93.68%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#print(prediction)\n",
    "test_labels=file['dataset'][0,0][0][0][0]\n",
    "#test_labels-=1  #corrijo porque empieza en 1\n",
    "testlabels=[]\n",
    "for i in file['dataset'][0,0][1][0][0][1]:\n",
    "    testlabels=np.append(testlabels,int(i))\n",
    "#print(file['dataset'][0,0][1][0][0][1])\n",
    "testlabels-=1 #corrijo pq empieza en 1 y debe empezar en 0\n",
    "\n",
    "#print(test.shape)\n",
    "#for i in test_labels:\n",
    " #   testlabels=np.append(testlabels,int(i[0]))\n",
    "#print(testlabels)\n",
    "#print(prediction)\n",
    "#print(np.where(prediction-testlabels==0)[0].shape[0])\n",
    "impr('{:.2f}'.format(np.where(prediction-testlabels==0)[0].shape[0]*100/testlabels.shape[0])+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
